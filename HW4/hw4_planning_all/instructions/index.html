<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Homework 4: Behavior Planning</title>
</head>

<body>
    <h1>Behavior Planning</h1>
    <p>
        In this assignment you will write an action planner using Heuristic Search Planning. Behavior Planners chain
        sequences of actions/behaviors together to solve a problem that requires looking multiple steps into the future.
        It is particularly good for solving puzzles.
    </p>
    <p>
        Specifically, you will be implementing the heuristic search planning (HSP) algorithm. HSP is, at its base,
        identical to A*. However, actions/behaviors are much more complex than in pathfinding and there is no obvious
        heuristic such as Euclidean distance, such as in pathfinding.
    </p>
    <p>
        In behavior planning, actions can be represented in STRIPS form. That is, each action has a name, preconditions,
        and effects. Preconditions are things that must be true in the world for an action to execute. Effects are the
        ways in which the world is changed if an action is executed. Preconditions and effects are given as logical
        propositions, e.g., "has_key" might be a symbol that represents the fact that an agent has a key.
    </p>
    <ol>
        <li>Action: Pickup_gold_in_chest</li>
        <li>Preconditions: at_chest, gold_in_chest, has_key</li>
        <li>Add_list: has_gold</li>
        <li>Delete_list: gold_in_chest </li>
    </ol>
    <p>
        In the example above, the agent has the option of picking up some gold in a locked chest. This action can only
        be performed if the agent is at the chest (at_chest), the gold is in the chest (gold_in_chest), and the agent
        has the key (has_key). The effects are split into two parts. The add_list are new facts that will be true about
        the world. In particular, the agent will have the gold (has_gold). The delete_list are those facts that will no
        longer be true (gold_in_chest).
    </p>
    <p>
        The objective of a planner is to determine which actions must be executed and in what order.
    </p>
    <p>
        However, how do we know if any action gets the agent closer to a goal. For example, the goal might be has_gold,
        in which case the action achieves the goal. But the goal might also be has_sword and having gold might be one
        step toward the goal of acquiring the sword (perhaps it can be bought at a store with gold). Does has_gold get
        us closer to the goal? It is not clear just by looking at the propositions whether the agent is one step away or
        ten steps away. Without being able to estimate how many more actions are needed to get to the goal, the best we
        can do is an exhaustive search such as breadth first search.
    </p>
    <p>
        Heuristic Search Planning adds a new element to A*: it allows us to analyze the actions to make an informed
        guess about how many actions it takes to get to a goal. HSP "relaxes" the planning problem in two ways. First,
        it pretends that the delete_lists don't exist. Delete_lists make things false and when things go from true to
        false then things get complicated. If things can only go from false to true, then propositions add up and it is
        only a matter of time before the agent has "collected" all the propositions to achieve a goal state. Second, HSP
        allows actions to happen simultaneously instead of in sequence, so we don't have to worry too much about whether
        one thing should happen before another thing. It turns out that with these two relaxation problems, planning is
        polynomial in time complexity instead of NP-hard. We can solve the relaxed problem in polynomial time and then
        use the number of steps in the simpler problem as a guess to how many steps, at a minimum, will be needed for
        the full planning problem. It will never take fewer steps than the solution to the simpler problem.
    </p>
    <p>
        HSP calculates the heuristic value for a given state in two stages. First it constructs a graph. The actions are
        nodes in the graph. There are two additional nodes: (1) a dummy action representing an empty goal transition in
        which the preconditions are the goal conditions and there is no add_list or delete_list, and (2) a dummy action
        representing an empty current state transition in which the add_list is the propositions true in the current
        state and with no preconditions or delete_list. Think of (1) as a special action for "cleaning up" and (2) as a
        special action for getting set up.
    </p>
    <p>
        To construct the graph, create an edge between action-nodes for each pair of actions where one of the add_list
        propositions in action 1 matches a proposition in the preconditions of action 2. Label the edge with the name of
        the proposition. Note that two actions might have multiple edges between them and that multiple edges
        terminating at the same action might have the same proposition label.
    </p>
    <p>
        The second step is to "walk" the graph and find the longest path between the dummy start action and the dummy
        end action. Create a queue. Add the dummy start to the queue and mark it as visited. Add any actions to the
        queue if all of their predecessors have been visited. If an action has multiple incoming edges with the same
        proposition, then only one of them must have been visited. This represents a step in the relaxed planning
        problem where all the preconditions have been met by a preceding action. Continue to pop and visit action-nodes
        in the queue. For each action-node visited, compute it's distance from the start as the dist = max(E_1, E_2,
        ...) for every incoming edge. E_i is a number we assign to each edge in the graph. For each outgoing edge we
        assign E_j = dist + 1 (or dist + cost if the action has a cost).
    </p>
    <p>
        When the queue is empty the heuristic value of getting from the current state (represented by the dummy start)
        to the goal state (represented by the dummy end) is the biggest edge cost observed.
    </p>
    <p>
        Now you can take any state that A* discovers and estimate the number of additional actions needed to get to the
        goal. Computing the heuristic is costly since it is a polynomial: it is O(n^2) where n is the number of possible
        actions. However, on average one should need to explore fewer states than an exhaustive breadth first search
        when using the generated HSP heuristic.
    </p>
    <p>
        Now implement A* as normal. The only difference between A* used for behavior planning and pathfinding is (1) the
        heuristic as described above, and (2) you need to use action add_lists and delete_lists to figure out the
        successor states. To generate a successor state, take an action. Check if the action is executable in the
        current state by checking to see if all of its preconditions are in the set of current state propositions. If
        so, then take the current state propositions, add all propositions in the action's add_list, and remove all
        propositions in the action's delete_list. This new set of propositions will be the new state that can be reached
        from the old state.
    </p>
    <hr />
    <h2>Things you need to know</h2>
    <p>
        Please consult previous homework instructions for background on the Game Engine. In addition to the information
        about the game engine provided there, the following are new elements you need to know about.
    </p>
    <h3><a href="statesactions.html#Action">Action class:</a></h3>
    <p>A container class for everything the planner needs to know about actions. It also contains logic for executing in
        the game engine.</p>
    <p>Member variables:</p>
    <ul>
        <li>name: string name for the action</li>
        <li>preconditions: a set (not a list) of propositions. Each proposition is a string that represents a single
            fact about the world that would need to be true for this action to work.</li>
        <li>add_list: a set (not a list) of propositions that become true if this action executes.</li>
        <li>delete_list: a set (not a list) of propostions that were once true but become false when the action is
            executed.</li>
        <li>cost: integer, how expensive the action is</li>
        <li>agent: pointer to the agent that would execute the action.</li>
        <li>first: records whether the execution loop (see below) has been executed more than once.</li>
    </ul>
    <p>Member functions:</p>
    <ul>
        <li>Reset: reset the action and get it ready for a new execution</li>
        <li>Execute: Execute this action in the game engine. If the action requires more than one tick, this function
            returns None. It returns True if the action completes successfully. It returns False if the action fails to
            complete.</li>
        <li>Enter: Runs the first time the execution function is called.</li>
    </ul>
    <h3><a href="statesactions.html#MoveAction">MoveAction class:</a></h3>
    <p>A subclass of Action that knows how to make the Agent move in the game engine.</p>
    <h3><a href="statesactions.html#DoorAction">DoorAction class:</a></h3>
    <p>A subclass of Action that handles opening doors in the game engine.</p>
    <h3><a href="statesactions.html#TriggerAction">TriggerAction class:</a></h3>
    <p>A subclass of Action that performs discrete triggering actions in the game engine (really anything that is not
        Move or opening doors).</p>
    <h3><a href="statesactions.html#State">State class:</a></h3>
    <p>A container for tracking everything the planner needs to know about a state during planning.</p>
    <p>Member variables:</p>
    <ul>
        <li>propositions: a set (not a list) of propositions, where each proposition is a string representing some fact
            about the world.</li>
        <li>g: integer, the cost from the initial state to the state represented by this object.</li>
        <li>h: integer, the estimated cost from this state to the goal state.</li>
        <li>parent: the state that this is a successor to.</li>
        <li>causing_action: the action (Action class object) that would be used to transition from the parent state to
            this state.</li>
        <li>id: string, a unique identifier</li>
    </ul>
    <p>Member functions:</p>
    <ul>
        <li>get_g: get the g-value</li>
        <li>get_h: get the h-value</li>
        <li>get_f: compute f = g + h</li>
    </ul>
    <h3><a href="planner.html">Planner class:</a></h3>
    <p>The class that does the planning. Implements <code>astar()</code> and <code>compute_heuristic()</code>. It is also meant to be used as a
        super-class for an agent object in the game engine.</p>
    <p>Member variables:
        You only need to know about the following:</p>
    <ul>
        <li>initial_state: the initial state (State object)</li>
        <li>goal_state: the goal state (State object)</li>
        <li>actions: a list of actions (Action objects)</li>
        <li>the_plan: a list of actions (Action objects) that will be executed.</li>
    </ul>
    <p>Member functions:
        You only need to know about the following:</p>
    <ul>
        <li>astar: to be completed. Takes an initial state (State object), goal state (State Object) and list of actions
            (Action objects). Returns two values: a plan as a list of Action objects, and the list of visited states
            called the closed list.</li>
        <li>compute_heuristic: to be completed. Takes a current state (State Object), goal state (State object), and
            list of actions (Action objects). Returns the heuristic value of transitioning from the current state to the
            goal state.</li>
        <li>update: called by the game engine every tick. When there is a plan (the_plan is not empty) then it executes
            each action in turn. </li>
    </ul>
    <p>Additional functions:</p>
    <ul>
        <li>is_goal: takes a state and a goal and checks if the state is a goal state.</li>
        <li>state_in_set: return true if the given state is in a set of states</li>
        <li>print_states: for debugging, print each state in a list of states</li>
    </ul>
    <h3><a href="npcworld.html#NPCAgent">NPCAgent class:</a></h3>
    <p>A type of Agent for this assignment. Doesn&#39;t do anything in particular of interest, except is sub-classes
        from Planner. The NPCAgent knows how to generate a plan at startup if it has an initial state and a goal. </p>
    <h3><a href="npcworld.html#Place">Place class:</a></h3>
    <p>A rectangular area in a game map of significance to the planning problem. The agent may need to go to one of
        these places. Also some actions can only be performed in some places.</p>
    <h3><a href="npcworld.html#DoorPlace">DoorPlace class:</a></h3>
    <p>A type of Place where doors can be opened.</p>
    <h3><a href="npcworld.html#NPCWorld">NPCWorld class:</a></h3>
    <p>A special type of GameWorld that maintains a ground-truth world state as a set (not a list) of propositions
        (strings where each represents a fact about the world) and knows how to check the preconditions of actions that
        are being executed.</p>

    <hr/>
    <h2>Instructions</h2>
    <ol>
        <li>
            <p>
            Implement A* for behavior planning. In <code>planner.py</code>, find the Planner class and complete the <code>astar()</code> function. The <code>astar()</code> function takes an initial state, a goal state, and a set of actions and produces a plan. A plan is an ordered list of actions. It must also return a list of visited states (in any order). In the codebase, there are classes for State and Action that you can use.
            </p>
            
        </li>
        <li>
            Implement the HSP heuristic function in the <code>compute_heuristic()</code> function in Planner. The <code>compute_heuristic()</code> function takes an arbitrary state and a goal state and returns an integer.
        </li>
        <p>
            TESTING: Use 
            <ul>
                <li><code>runromania.py</code> (see the <a href="romania.pdf">map</a>)</li>
                <li><code>rundoor.py</code></li>
                <li><code>runbank.py</code></li>
                <li><code>runblocks.py</code></li>
                <li><code>runcook.py</code></li>
                <li><code>runnotzelda.py</code></li>
            </ul>
            You can also test the planner with an agent in <code>rundoormap.py</code>, <code>runbankmap.py</code>, and <code>runnotzeldamap.py</code> to see the agent executing in a graphical world. You will find it easier to understand <code>runnotzelda.py</code> by first running <code>runnotzeldamap.py</code>.
        </p>
    </ol>
    <hr/>
    <h2>Grading</h2>
    <p>Grading occurs in two stages</p>
    <p>
        <b>Stage 1 (5 points):</b> Grading of your implementation of <code>compute_heuristic()</code> will be done on test states similar to those in <code>run____.py</code> by comparing computed heuristics with expected heuristics.
    </p>
    <p>
        <b>Stage 2 (5 points):</b> Grading of your implementation of <code>astar()</code> will be done on test states similar to those in <code>run____.py</code> by comparing computed plans and visited states with expected plans and visited states.
    </p>
    <p>There are different numbers of test in each of the maps. Each heuristic test is scored at 1/(total number of tests). Each astar test is scored at 1/(total number of tests).
    </p>
    <hr/>
    <h2>Hints</h2>
    <p>Initially, implement A* with a h(s) = 0 heuristic. This will operate like breadth-first search and be inefficient with number of visited states, but if you can get the optimal plan with the empty heuristic, then you can then go on to implement compute_heuristic.</p>
    <p>The not-zelda problem is the most complicated. To understand what is going on in runnotzelda.py, you will first want to play runnotzeldamap.py. This is a version that has the user playing the green agent and an NPC playing the purple agent (initially off the screen to the right). You can click to move your agent around. Press "t" to trigger an action associated with a location. For example if you are at "table_7" and press "t", then you will pick up "key_7" and if you are at the crack, and you have key_4 or key_3, you will drop the key and it will become available on the other side of the crack. When you press "p", the NPC will attempt to generate and execute a plan. Initially it will fail as key_3 or key_4 are not on the right side of the map. If you want to do everything yourself, you can. Your agent can traverse from the left side of the map to the right side of the map if you click on the right side of the map across from table_7 and table_8. The NPC cannot move to the left side of the map. runnotzelda.py is slightly different in that it has a single AI agent that can do both sides of the map, starting on the left and able to get all keys on the left before moving to the right.
    </p>
    <h2>Submission</h2>
    <p>
        To submit your solution, upload your modified <code>planner.py</code> file to Gradescope.
    </p>
    <p>
        You should not modify any other files in the game engine.
    </p>
    <p>
        DO NOT upload the entire game engine.
    </p>
</body>

</html>